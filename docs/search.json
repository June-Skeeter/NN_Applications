[
  {
    "objectID": "about.html#eddy-covariance",
    "href": "about.html#eddy-covariance",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Eddy Covariance",
    "text": "Eddy Covariance\n\n\nSemi-continuous, ecosystem-scale energy, water, and trace gas fluxes.\n\nNoisy, voluminous data sets\n\nFrequent gaps\nObservational bias\n\nWell suited for machine learning!\n\n\n\n\nBurns Bog EC StationDelta, BC"
  },
  {
    "objectID": "about.html#neural-networks",
    "href": "about.html#neural-networks",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Neural Networks",
    "text": "Neural Networks\nUniversal approximators: can map any continuous function to an arbitrary degree accuracy.\n\nWith enough hidden nodes, will fit any pattern in a dataset\n\nCare must be taken to ensure the patterns are real\nEarly stopping can help prevent over-fitting\n\nWell suited for non-linear, multi-variate response functions\n\nCapable of interpolation and extrapolation"
  },
  {
    "objectID": "about.html#commonly-cited-limitations",
    "href": "about.html#commonly-cited-limitations",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Commonly Cited Limitations",
    "text": "Commonly Cited Limitations\n\n\n\n\n\n\n\n\n\nIssue\nSolutions\n\n\n\n\nOver-fitting\n- Model ensembles- 3-way cross validation - Pruning inputs\n\n\nBlack boxes models\n- Plot partial derivatives - Feature importance\n\n\nComputationally expensive\n- Tensorflow - GPU processing"
  },
  {
    "objectID": "about.html#example-data",
    "href": "about.html#example-data",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Example Data",
    "text": "Example Data\n\n\nBurns Bog EC station\n\nHarvested peatland undergoing active restoration\n8+ years of flux data"
  },
  {
    "objectID": "about.html#training-procedures",
    "href": "about.html#training-procedures",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Training Procedures",
    "text": "Training Procedures\n\n\n\nLarger ensemble = more robust model\n\nN \\(\\leq\\) 10 for data exploration/pruning\n\nThree way cross-validation\n\nTrain/validate/test\n\nEarly Stopping: after e epochs\n\ne = 2 for pruning stage"
  },
  {
    "objectID": "about.html#pruning-inputs",
    "href": "about.html#pruning-inputs",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Pruning Inputs",
    "text": "Pruning Inputs\nCalculate partial first derivative of the output with respect to each input over test data domain.\n\nRelative Influence (RI) of inputs\n\nNormalized sum of squared derivatives (SSD)\n\nIteratively remove inputs with RI below a threshold\n\nUse random noise to determine threshold\n\ne.g., randomly shuffled copy of each input"
  },
  {
    "objectID": "about.html#before-and-after-pruning-fco2",
    "href": "about.html#before-and-after-pruning-fco2",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Before and After Pruning FCO2",
    "text": "Before and After Pruning FCO2"
  },
  {
    "objectID": "about.html#the-final-model",
    "href": "about.html#the-final-model",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "The Final Model",
    "text": "The Final Model\nOnce pruning is complete, re-train the final production level model, excluding the random scalars\n\nIncrease the ensemble size (e.g., N \\(\\geq\\) 30)\n\nIncrease early stopping (e) criteria (e.g., e = 10)\n\nLarger e drastically increases training time\n\n\nPlot the model derivatives as a final check\n\nIf derivatives look implausible\n\nAdjust inputs/parameters and try again"
  },
  {
    "objectID": "about.html#plotting-derivatives",
    "href": "about.html#plotting-derivatives",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Plotting Derivatives",
    "text": "Plotting Derivatives\nHelps ensure model responses are physically plausible\n\nAn essential step and key advantage of NN models\nRaw derivatives show response in natural units\nNormalized derivatives scaled by input variance\n\nRelative input effects on common scale\nWhat the model “sees”"
  },
  {
    "objectID": "about.html#partial-derivatives-of-fco2",
    "href": "about.html#partial-derivatives-of-fco2",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Partial Derivatives of FCO2",
    "text": "Partial Derivatives of FCO2"
  },
  {
    "objectID": "about.html#normalized-derivatives-of-fco2",
    "href": "about.html#normalized-derivatives-of-fco2",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Normalized Derivatives of FCO2",
    "text": "Normalized Derivatives of FCO2"
  },
  {
    "objectID": "about.html#model-performance-fco2",
    "href": "about.html#model-performance-fco2",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Model Performance FCO2",
    "text": "Model Performance FCO2\n\n\nPlot the model outputs and validation metrics calculated with the test data.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nScore\n\n\n\n\nRMSE\n0.64 \\(\\mu mol\\) \\(m^{-2}s^{-1}\\)\n\n\nr2\n0.88"
  },
  {
    "objectID": "about.html#before-and-after-pruning-fch4",
    "href": "about.html#before-and-after-pruning-fch4",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Before and After Pruning FCH4",
    "text": "Before and After Pruning FCH4"
  },
  {
    "objectID": "about.html#normalized-derivatives-of-fch4",
    "href": "about.html#normalized-derivatives-of-fch4",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Normalized Derivatives of FCH4",
    "text": "Normalized Derivatives of FCH4"
  },
  {
    "objectID": "about.html#model-performance-fch4",
    "href": "about.html#model-performance-fch4",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Model Performance FCH4",
    "text": "Model Performance FCH4\n\n\nPlot the model outputs and validation metrics calculated with the test data.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nScore\n\n\n\n\nRMSE\n17.17 \\(nmol\\) \\(m^{-2}s^{-1}\\)\n\n\nr2\n0.89"
  },
  {
    "objectID": "about.html#next-steps",
    "href": "about.html#next-steps",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Next Steps",
    "text": "Next Steps\n\nCustom NN architecture: Separating input layers may allow us partition fluxes.\n\ne.g., FCO2 into GPP and ER\n\nFlux footprints: map response to spatial heterogenity\nUpscaling: in space and time\nu* filtering: partial derivatives could identify u* thresholds\nCompare to process based models (e.g., CLASSIC)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "",
    "text": "Abstract\nEddy covariance (EC) is used to monitor fluxes of energy, water, and carbon in ecosystems across the globe. The assumptions underpinning EC are not valid under all meteorologic or operational conditions, which results in an observation bias that must be considered when working with EC data. Trace gas fluxes exhibit spatially and temporally variable, non-linear dependence upon multiple drivers. Multi-year EC data sets have hundreds of thousands of data points and flux time series contain both noise and data gaps. These factors make EC data well suited for analysis with neural network (NN) models, a flexible machine learning method for mapping relationships in large multivariate data sets with non-linear dependencies. Often NN models have been treated as black box models. However, careful inspection of model derivatives provides a method for ensuring that relationships mapped by a NN are physically plausible. Customizing the structure of a model can also help guarantee the model is emulating real world phenomena. Here we present guidance for applying NN models to EC data and provide a corresponding GitHub repository with functional NN examples written in Python. We use carbon dioxide and methane flux data from wetland sites in southwestern British Columbia to demonstrate how model derivatives can be used to detect, visualize, and rank the importance of functional relationships driving fluxes. We also show how NNs can be used for gap-filling and upscaling, and we offer comparisons to other common machine learning methods like random forests."
  },
  {
    "objectID": "NN_for_EC.html",
    "href": "NN_for_EC.html",
    "title": "Neural Networks for Eddy Covariance",
    "section": "",
    "text": "## Import some standard packages and define a few functions\nimport os\n# Hide default info, logs, and warnings - comment out if you need to troubleshoot\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport time\nimport shutil\nimport importlib\nimport numpy as np\nimport pandas as pd\n# from matplotlib import cm\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom Scripts import PlotHelpers\n# from matplotlib.colors import Normalize\nfrom Scripts import ReadDB, MiscFuncs, NNetFuncs\n\n# dbNames = {\n#     'Clean/SecondStage/TA_1_1_1':'T air',\n#     'Clean/SecondStage/RH_1_1_1':'RH',\n#     'Clean/SecondStage/FC':'FCO2',\n#     'Clean/SecondStage/FCH4':'FCH4',\n#     'Clean/SecondStage/PPFD_IN_1_1_1':'PPFD',\n#     'Clean/SecondStage/NETRAD_1_1_1':'Rn',\n#     'Clean/SecondStage/P_1_1_1':'Precip',\n#     'Flux/qc_co2_flux':'qc_FCO2',\n#     'Flux/qc_ch4_flux':'qc_FCH4',\n#     'Clean/SecondStage/USTAR':'u*',\n#     'Clean/SecondStage/TS_1':'T soil 5cm',\n#     'Clean/SecondStage/TS_2':'T soil 10cm',\n#     'Clean/SecondStage/TS_3':'T soil 50cm',\n#     'Clean/SecondStage/wind_speed':'Wind speed',\n#     'Clean/SecondStage/wind_dir':'Wind dir',\n#     'Clean/SecondStage/WTD_1_1_1':'Water table',\n# }\n\n# Local = '/mnt/c/Users/User/PostDoc_Work/database/'\n# Remote = '/mnt/w/'\n\n# Dir = Local\n\n\n# read_new = True\n# if read_new == True:\n#     Data = ReadDB.get_Traces(Site,list(dbNames.keys()),Dir=Dir)\n#     Data = Data.rename(columns=dbNames)\n#     Data.to_csv(f'InputData/{Site}_Data.csv')\n\n# else:\n#     Data = pd.read_csv(f'InputData/{Site}_Data.csv',parse_dates=['TIMESTAMP'],index_col='TIMESTAMP')\n\n\n\n\nSite = 'BB'\nData = pd.read_csv(f'InputData/NN_Data_{Site}.csv',parse_dates=['TIMESTAMP'],index_col='TIMESTAMP')\n# BB2 = pd.read_csv('InputData/NN_Data_BB2.csv',parse_dates=['TIMESTAMP'],index_col='TIMESTAMP')\n# def Q_clip(df,trace,q=[.001,.999]):\n#     qv=df[trace].quantile(q).values\n#     df.loc[((df[trace]<qv[0])|df[trace]>qv[-1]),trace]=np.nan\n#     return(df[trace])\n\nData = Data.loc[Data.index.year>=2018].copy()\n\nVars = {'WTD_1_1_1':'Water table',\n     'TA_1_1_1':'T air',\n     'RH_1_1_1':'RH',\n     'FC':'FCO2',\n     'FCH4':'FCH4',\n     'PPFD_IN_1_1_1':'PPFD',\n     'NETRAD_1_1_1':'Rn',\n     'P_1_1_1':'Precip',\n     'qc_co2_flux':'qc_FCO2',\n     'qc_ch4_flux':'qc_FCH4',\n     'USTAR':'u*',\n     'TS_1':'T soil 5cm',\n     'TS_2':'T soil 10cm',\n     'TS_3':'T soil 50cm',\n     'WS_1_1_1':'Wind speed',\n     'WD_1_1_1':'Wind dir'}\n\nData = Data.rename(columns=Vars)\nreff = Data.loc[Data.index.date==pd.Timestamp('2020-03-18').date(),'Water table'].mean()\nData['Water table'] = Data['Water table'] - reff\n# BB1 = BB1.rename(columns=Vars)\n# BB2 = BB2.rename(columns=Vars)\n# BB1['Site'] = 1\n# BB1_reff = BB1.loc[BB1.index.date==pd.Timestamp('2020-03-18').date(),'Water table'].mean()\n# BB1['Water table'] = BB1['Water table'] - BB1_reff\n# BB2['Site'] = 2\n# BB2_reff = BB2.loc[BB2.index.date==pd.Timestamp('2020-03-18').date(),'Water table'].mean()\n# BB2['Water table'] = BB2['Water table'] - BB2_reff\n\n# bearing = 166\n# BB1.loc[BB1['Wind dir']-bearing]\n\n# Data = pd.concat([BB1,BB2])\nData = Data.drop(columns=['SW_IN_1_1_1','LW_IN_1_1_1','SW_OUT_1_1_1','LW_OUT_1_1_1','VPD_1_1_1','PA_1_1_1'])\nprint(Data.shape)\nData = Data.dropna(how='any')\nprint(Data.shape)\n\n(105169, 16)\n(29988, 16)\n\n\n\n\n\nimportlib.reload(ReadDB)\n\nData['VPD'] = MiscFuncs.Calc_VPD(Data['T air'],Data['RH'])\nData['Water table'] = -1*(70-Data['Water table'])\nData['DOY'] = Data.index.dayofyear\n\ntarget = ['FCO2','FCH4']\n\nData['Rand']=np.random.random(Data['FCO2'].values.shape)\nData['Rand_Binary'] = Data['Rand']-.5\nData['Rand_Skew'] = Data['Rand']**.25\nData.loc[Data['Rand_Binary']>0,'Rand_Binary']=1\nData.loc[Data['Rand_Binary']<0,'Rand_Binary']=-1\nRand_Scalars=['Rand','Rand_Binary','Rand_Skew']\n\nprint(Data[['FCO2','FCH4']].describe())\nfilter = ReadDB.filterFlux(Data,target)\nfilter.QA_QC()\n# filter.dir_mask('Wind dir',[[0,45],[315,360]])\nfilter.rain('Precip',thresh=0)\n# filter.MAD(z=5)\nfilter.uStar('u*',u_thresh=0.1)\n\nData[['FCO2_Clean','FCH4_Clean']] = filter.df[['FCO2','FCH4']].copy()\n\nprint(Data[['FCO2_Clean','FCH4_Clean']].describe())\n\nexcludes = ['fco2','fch4','precip']\n\nFull_inputs = []\n\nfor val in list(Data.columns):\n    exct = 0\n    for ex in excludes:\n        if ex in val.lower():\n            exct += 1\n    if exct < 1:\n        Full_inputs.append(val)\n\nData.count()\n\n               FCO2          FCH4\ncount  29988.000000  29988.000000\nmean      -0.455273     48.452158\nstd        2.314013     65.492819\nmin      -36.371260   -198.555830\n25%       -1.783542      9.644733\n50%       -0.224877     31.494045\n75%        0.666671     82.179884\nmax       45.147030    686.522200\n         FCO2_Clean    FCH4_Clean\ncount  15396.000000  17450.000000\nmean      -1.196605     55.352024\nstd        1.856238     49.478366\nmin      -16.697300   -176.868300\n25%       -2.545481     13.166115\n50%       -1.185378     40.418017\n75%        0.259368     93.315557\nmax       23.026184    514.325700\n\n\nRn             29988\nPPFD           29988\nT air          29988\nT soil 5cm     29988\nT soil 10cm    29988\nT soil 50cm    29988\nRH             29988\nPrecip         29988\nWind speed     29988\nWind dir       29988\nWater table    29988\nu*             29988\nFCO2           29988\nFCH4           29988\nqc_FCO2        29988\nqc_FCH4        29988\nVPD            29988\nDOY            29988\nRand           29988\nRand_Binary    29988\nRand_Skew      29988\nFCO2_Clean     15396\nFCH4_Clean     17450\ndtype: int64\n\n\n\nfig,ax=plt.subplots()\nax.scatter(Data['FCO2'],Data['PPFD'])\nax.scatter(Data['FCO2_Clean'],Data['PPFD'],facecolors='none', edgecolors='r')\nData.loc[((Data['FCO2_Clean']<0)&(Data['PPFD']<5))]\n\n\n\n\n\n  \n    \n      \n      Rn\n      PPFD\n      T air\n      T soil 5cm\n      T soil 10cm\n      T soil 50cm\n      RH\n      Precip\n      Wind speed\n      Wind dir\n      ...\n      FCH4\n      qc_FCO2\n      qc_FCH4\n      VPD\n      DOY\n      Rand\n      Rand_Binary\n      Rand_Skew\n      FCO2_Clean\n      FCH4_Clean\n    \n    \n      TIMESTAMP\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2018-09-07 01:00:00\n      -43.128560\n      0.008049\n      10.359350\n      16.032904\n      15.806022\n      15.322218\n      95.989540\n      0.0\n      0.447382\n      182.968890\n      ...\n      109.246170\n      0.0\n      0.0\n      5.063089e-01\n      250\n      0.364599\n      -1.0\n      0.777059\n      -0.251189\n      109.246170\n    \n    \n      2018-09-07 21:30:00\n      -5.673309\n      0.010729\n      14.604237\n      15.951805\n      15.627492\n      15.252795\n      98.143490\n      0.0\n      0.989659\n      86.495705\n      ...\n      91.680466\n      0.0\n      0.0\n      3.097148e-01\n      250\n      0.586010\n      1.0\n      0.874936\n      -0.692962\n      91.680466\n    \n    \n      2018-09-08 19:30:00\n      -17.488834\n      0.064360\n      15.771963\n      16.176157\n      15.903015\n      15.447454\n      95.503620\n      0.0\n      0.905848\n      27.063473\n      ...\n      62.703990\n      0.0\n      0.0\n      8.085810e-01\n      251\n      0.074613\n      -1.0\n      0.522640\n      -1.475593\n      62.703990\n    \n    \n      2018-09-08 20:00:00\n      -15.425125\n      0.058013\n      15.348302\n      16.163733\n      15.850715\n      15.435830\n      95.449990\n      0.0\n      0.569989\n      161.495360\n      ...\n      66.677345\n      0.0\n      0.0\n      7.963082e-01\n      251\n      0.004228\n      -1.0\n      0.254997\n      -1.900765\n      66.677345\n    \n    \n      2018-09-08 20:30:00\n      -12.295860\n      0.013409\n      15.044191\n      16.164623\n      15.738058\n      15.450691\n      96.822020\n      0.0\n      1.152366\n      135.747680\n      ...\n      70.325740\n      0.0\n      0.0\n      5.454207e-01\n      251\n      0.917991\n      1.0\n      0.978835\n      -1.721321\n      70.325740\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-16 23:00:00\n      -75.242165\n      0.000000\n      5.555014\n      3.498761\n      3.629818\n      6.416536\n      79.725365\n      0.0\n      4.218254\n      319.816000\n      ...\n      2.240025\n      0.0\n      0.0\n      1.845651e+00\n      47\n      0.575242\n      1.0\n      0.870889\n      -1.093810\n      2.240025\n    \n    \n      2021-02-22 21:00:00\n      -36.564200\n      0.000000\n      6.883492\n      6.075706\n      5.752314\n      6.460669\n      75.322860\n      0.0\n      4.409684\n      308.870450\n      ...\n      -0.343134\n      0.0\n      1.0\n      2.462149e+00\n      53\n      0.185709\n      -1.0\n      0.656460\n      -0.975886\n      NaN\n    \n    \n      2022-01-27 00:30:00\n      -4.966819\n      0.346958\n      1.410606\n      5.505546\n      5.795523\n      7.158379\n      100.000000\n      0.0\n      1.002757\n      18.365236\n      ...\n      5.716230\n      0.0\n      1.0\n      -1.110223e-15\n      27\n      0.202556\n      -1.0\n      0.670867\n      -0.438759\n      NaN\n    \n    \n      2023-01-22 00:30:00\n      -34.673756\n      0.000000\n      4.101948\n      5.547119\n      5.923764\n      7.107648\n      96.597960\n      0.0\n      1.953380\n      331.442750\n      ...\n      18.144798\n      0.0\n      1.0\n      2.798234e-01\n      22\n      0.491008\n      -1.0\n      0.837090\n      -0.784649\n      NaN\n    \n    \n      2023-03-24 23:30:00\n      -5.295106\n      0.005812\n      3.700751\n      8.030537\n      8.247041\n      7.808859\n      100.000000\n      0.0\n      1.226768\n      59.598522\n      ...\n      -36.799400\n      0.0\n      1.0\n      0.000000e+00\n      83\n      0.924235\n      1.0\n      0.980495\n      -0.735831\n      NaN\n    \n  \n\n71 rows × 23 columns"
  },
  {
    "objectID": "NN_for_EC.html#build-and-train-model",
    "href": "NN_for_EC.html#build-and-train-model",
    "title": "Neural Networks for Eddy Covariance",
    "section": "Build and train model",
    "text": "Build and train model\n\nimportlib.reload(NNetFuncs)\n\n\ndef Build_Train_Eval(Run,print_sum=False):\n\n    config = Run['config']\n    Training = Run['Training']\n    NNetFuncs.make_Dense_model(config,print_sum=print_sum)\n    Eval=NNetFuncs.train_model(config,Training)\n    _=NNetFuncs.run_Model(config,Eval)\n\nfor Run in Model_Runs.keys():\n    print(Run)\n    Build_Train_Eval(Model_Runs[Run],print_sum=True)\n    print('\\n\\n')\n\nFull_Model_FCH4\nModel: \"Full_Model_FCH4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 17)]              0         \n                                                                 \n normalization (Normalizatio  (None, 17)               0         \n n)                                                              \n                                                                 \n dense (Dense)               (None, 170)               3060      \n                                                                 \n dense_1 (Dense)             (None, 1)                 171       \n                                                                 \n=================================================================\nTotal params: 3,231\nTrainable params: 3,231\nNon-trainable params: 0\n_________________________________________________________________\nNone\n(12923,) 0\n(12923,) 0\n(12923,) 0\n(12923,) 0\n(12923,) 0\n(12923,) 0\n(12923,) 0\n(12923,) 0\n(12923,) 0\n(12923,) 0\nTraining Time:\n 102.1  Seconds\n(4308,)\n(10, 4308, 1)\n\n\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'dy_d{xi}_norm_CI95']=full_out['dy_dx_norm'].std(axis=0)[:,i]/(N)**.5*t_score\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'dy_d{xi}_norm']=full_out['dy_dx_norm'].mean(axis=0)[:,i]\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'{xi}_norm']=full_out['X_norm'].mean(axis=0)[:,i]\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'dy_d{xi}_norm_CI95']=full_out['dy_dx_norm'].std(axis=0)[:,i]/(N)**.5*t_score\n\n\nNN Model\n Validation metrics (ensemble mean): \nr2 =  0.5698 \nRMSE =  9.52423\nRun Time:\n 1.03  Seconds\n10 models\nMean epochs/model:  64.0\n\n\n\nFull_Model_FCO2\nModel: \"Full_Model_FCO2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 17)]              0         \n                                                                 \n normalization (Normalizatio  (None, 17)               0         \n n)                                                              \n                                                                 \n dense (Dense)               (None, 170)               3060      \n                                                                 \n dense_1 (Dense)             (None, 1)                 171       \n                                                                 \n=================================================================\nTotal params: 3,231\nTrainable params: 3,231\nNon-trainable params: 0\n_________________________________________________________________\nNone\n(17059,) 0\n(17059,) 0\n(17059,) 0\n(17059,) 0\n(17059,) 0\n(17059,) 0\n(17059,) 0\n(17059,) 0\n(17059,) 0\n(17059,) 0\nTraining Time:\n 56.89  Seconds\n(5687,)\n(10, 5687, 1)\n\n\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'dy_d{xi}_norm_CI95']=full_out['dy_dx_norm'].std(axis=0)[:,i]/(N)**.5*t_score\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'dy_d{xi}_norm']=full_out['dy_dx_norm'].mean(axis=0)[:,i]\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'{xi}_norm']=full_out['X_norm'].mean(axis=0)[:,i]\n/home/jskeeter/NN_Applications/Scripts/NNetFuncs.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  Mean_Output[f'dy_d{xi}_norm_CI95']=full_out['dy_dx_norm'].std(axis=0)[:,i]/(N)**.5*t_score\n\n\nNN Model\n Validation metrics (ensemble mean): \nr2 =  0.88828 \nRMSE =  0.46135\nRun Time:\n 1.53  Seconds\n10 models\nMean epochs/model:  39.2"
  },
  {
    "objectID": "LSTM_example.html",
    "href": "LSTM_example.html",
    "title": "Neural Networks for Eddy Covariance",
    "section": "",
    "text": "https://keras.io/examples/timeseries/timeseries_weather_forecasting/\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n\n2023-06-02 13:41:12.753634: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n\n\n\nfrom zipfile import ZipFile\nimport os\n\nuri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\nzip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\nzip_file = ZipFile(zip_path)\nzip_file.extractall()\ncsv_path = \"jena_climate_2009_2016.csv\"\n\ndf = pd.read_csv(csv_path)\n\n\ntitles = [\n    \"Pressure\",\n    \"Temperature\",\n    \"Temperature in Kelvin\",\n    \"Temperature (dew point)\",\n    \"Relative Humidity\",\n    \"Saturation vapor pressure\",\n    \"Vapor pressure\",\n    \"Vapor pressure deficit\",\n    \"Specific humidity\",\n    \"Water vapor concentration\",\n    \"Airtight\",\n    \"Wind speed\",\n    \"Maximum wind speed\",\n    \"Wind direction in degrees\",\n]\n\nfeature_keys = [\n    \"p (mbar)\",\n    \"T (degC)\",\n    \"Tpot (K)\",\n    \"Tdew (degC)\",\n    \"rh (%)\",\n    \"VPmax (mbar)\",\n    \"VPact (mbar)\",\n    \"VPdef (mbar)\",\n    \"sh (g/kg)\",\n    \"H2OC (mmol/mol)\",\n    \"rho (g/m**3)\",\n    \"wv (m/s)\",\n    \"max. wv (m/s)\",\n    \"wd (deg)\",\n]\n\ncolors = [\n    \"blue\",\n    \"orange\",\n    \"green\",\n    \"red\",\n    \"purple\",\n    \"brown\",\n    \"pink\",\n    \"gray\",\n    \"olive\",\n    \"cyan\",\n]\n\ndate_time_key = \"Date Time\"\n\n\ndef show_raw_visualization(data):\n    time_data = data[date_time_key]\n    fig, axes = plt.subplots(\n        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n    )\n    for i in range(len(feature_keys)):\n        key = feature_keys[i]\n        c = colors[i % (len(colors))]\n        t_data = data[key]\n        t_data.index = time_data\n        t_data.head()\n        ax = t_data.plot(\n            ax=axes[i // 2, i % 2],\n            color=c,\n            title=\"{} - {}\".format(titles[i], key),\n            rot=25,\n        )\n        ax.legend([titles[i]])\n    plt.tight_layout()\n\n\nshow_raw_visualization(df)\n\n\n\n\n\ndef show_heatmap(data):\n    plt.matshow(data.corr())\n    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n    plt.gca().xaxis.tick_bottom()\n    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n    plt.show()\n\n\nshow_heatmap(df)\n\n/tmp/ipykernel_6034/97386125.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  plt.matshow(data.corr())\n\n\n\n\n\n\nsplit_fraction = 0.715\ntrain_split = int(split_fraction * int(df.shape[0]))\nstep = 6\n\npast = 720\nfuture = 72\nlearning_rate = 0.001\nbatch_size = 256\nepochs = 10\n\n\ndef normalize(data, train_split):\n    data_mean = data[:train_split].mean(axis=0)\n    data_std = data[:train_split].std(axis=0)\n    return (data - data_mean) / data_std\n\n\nprint(\n    \"The selected parameters are:\",\n    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n)\nselected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\nfeatures = df[selected_features]\nfeatures.index = df[date_time_key]\nfeatures.head()\n\nfeatures = normalize(features.values, train_split)\nfeatures = pd.DataFrame(features)\nfeatures.head()\n\ntrain_data = features.loc[0 : train_split - 1]\nval_data = features.loc[train_split:]\n\nThe selected parameters are: Pressure, Temperature, Saturation vapor pressure, Vapor pressure deficit, Specific humidity, Airtight, Wind speed\n\n\n\nstart = past + future\nend = start + train_split\n\nx_train = train_data[[i for i in range(7)]].values\ny_train = features.iloc[start:end][[1]]\n\nsequence_length = int(past / step)\n\n\ndataset_train = keras.preprocessing.timeseries_dataset_from_array(\n    x_train,\n    y_train,\n    sequence_length=sequence_length,\n    sampling_rate=step,\n    batch_size=batch_size,\n)\n\n2023-06-02 13:41:35.670439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:35.757268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:35.757395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:35.759039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-06-02 13:41:35.763698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:35.763900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:35.763932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:37.582587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:37.583282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:37.583318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n2023-06-02 13:41:37.583486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-06-02 13:41:37.584337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3447 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n\n\n\nx_end = len(val_data) - past - future\n\nlabel_start = train_split + past + future\n\nx_val = val_data.iloc[:x_end][[i for i in range(7)]].values\ny_val = features.iloc[label_start:][[1]]\n\ndataset_val = keras.preprocessing.timeseries_dataset_from_array(\n    x_val,\n    y_val,\n    sequence_length=sequence_length,\n    sampling_rate=step,\n    batch_size=batch_size,\n)\n\n\nfor batch in dataset_train.take(1):\n    inputs, targets = batch\n\nprint(\"Input shape:\", inputs.numpy().shape)\nprint(\"Target shape:\", targets.numpy().shape)\n\nInput shape: (256, 120, 7)\nTarget shape: (256, 1)\n\n\n\ninputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\nlstm_out = keras.layers.LSTM(32)(inputs)\noutputs = keras.layers.Dense(1)(lstm_out)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 120, 7)]          0         \n                                                                 \n lstm (LSTM)                 (None, 32)                5120      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 5,153\nTrainable params: 5,153\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\npath_checkpoint = \"model_checkpoint.h5\"\nes_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n\nmodelckpt_callback = keras.callbacks.ModelCheckpoint(\n    monitor=\"val_loss\",\n    filepath=path_checkpoint,\n    verbose=1,\n    save_weights_only=True,\n    save_best_only=True,\n)\n\nhistory = model.fit(\n    dataset_train,\n    epochs=epochs,\n    validation_data=dataset_val,\n    callbacks=[es_callback, modelckpt_callback],\n)\n\nEpoch 1/10\n\n\n2023-06-02 13:41:41.770789: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n2023-06-02 13:41:43.177322: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n\n\n1170/1172 [============================>.] - ETA: 0s - loss: 0.2093\nEpoch 1: val_loss improved from inf to 0.15566, saving model to model_checkpoint.h5\n1172/1172 [==============================] - 43s 33ms/step - loss: 0.2091 - val_loss: 0.1557\nEpoch 2/10\n1170/1172 [============================>.] - ETA: 0s - loss: 0.1272\nEpoch 2: val_loss did not improve from 0.15566\n1172/1172 [==============================] - 28s 24ms/step - loss: 0.1272 - val_loss: 0.1582\nEpoch 3/10\n1171/1172 [============================>.] - ETA: 0s - loss: 0.1138\nEpoch 3: val_loss did not improve from 0.15566\n1172/1172 [==============================] - 29s 25ms/step - loss: 0.1138 - val_loss: 0.1606\nEpoch 4/10\n1172/1172 [==============================] - ETA: 0s - loss: 0.1105\nEpoch 4: val_loss did not improve from 0.15566\n1172/1172 [==============================] - 32s 27ms/step - loss: 0.1105 - val_loss: 0.1586\nEpoch 5/10\n1170/1172 [============================>.] - ETA: 0s - loss: 0.1090\nEpoch 5: val_loss improved from 0.15566 to 0.15451, saving model to model_checkpoint.h5\n1172/1172 [==============================] - 32s 27ms/step - loss: 0.1090 - val_loss: 0.1545\nEpoch 6/10\n1172/1172 [==============================] - ETA: 0s - loss: 0.1139\nEpoch 6: val_loss improved from 0.15451 to 0.13558, saving model to model_checkpoint.h5\n1172/1172 [==============================] - 33s 28ms/step - loss: 0.1139 - val_loss: 0.1356\nEpoch 7/10\n1169/1172 [============================>.] - ETA: 0s - loss: 0.1050\nEpoch 7: val_loss did not improve from 0.13558\n1172/1172 [==============================] - 28s 24ms/step - loss: 0.1049 - val_loss: 0.1397\nEpoch 8/10\n1169/1172 [============================>.] - ETA: 0s - loss: 0.1053\nEpoch 8: val_loss improved from 0.13558 to 0.13464, saving model to model_checkpoint.h5\n1172/1172 [==============================] - 30s 25ms/step - loss: 0.1052 - val_loss: 0.1346\nEpoch 9/10\n1172/1172 [==============================] - ETA: 0s - loss: 0.1060\nEpoch 9: val_loss improved from 0.13464 to 0.13090, saving model to model_checkpoint.h5\n1172/1172 [==============================] - 30s 26ms/step - loss: 0.1060 - val_loss: 0.1309\nEpoch 10/10\n1171/1172 [============================>.] - ETA: 0s - loss: 0.1088\nEpoch 10: val_loss improved from 0.13090 to 0.12816, saving model to model_checkpoint.h5\n1172/1172 [==============================] - 35s 30ms/step - loss: 0.1088 - val_loss: 0.1282\n\n\n\ndef visualize_loss(history, title):\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    epochs = range(len(loss))\n    plt.figure()\n    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n    plt.title(title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\nvisualize_loss(history, \"Training and Validation Loss\")\n\n\n\n\n\ndef show_plot(plot_data, delta, title):\n    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n    marker = [\".-\", \"rx\", \"go\"]\n    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n    if delta:\n        future = delta\n    else:\n        future = 0\n\n    plt.title(title)\n    for i, val in enumerate(plot_data):\n        if i:\n            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n        else:\n            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n    plt.legend()\n    plt.xlim([time_steps[0], (future + 5) * 2])\n    plt.xlabel(\"Time-Step\")\n    plt.show()\n    return\n\n\nfor x, y in dataset_val.take(5):\n    show_plot(\n        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n        12,\n        \"Single Step Prediction\",\n    )\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n8/8 [==============================] - 0s 12ms/step\n\n\n\n\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n8/8 [==============================] - 0s 10ms/step\n\n\n\n\n\n8/8 [==============================] - 0s 12ms/step\n\n\n\n\n\n\nprint(dataset_val.take(2))\n\nfor x, y in dataset_val.take(1):\n  print(x.shape)\n  \n\n  with tf.GradientTape(persistent=True,watch_accessed_variables=True) as tape:\n    tape.watch(x)\n    yo = model(x)\n      \n  print(yo.shape,tape.gradient(yo,x).numpy().shape)\n\n<TakeDataset element_spec=(TensorSpec(shape=(None, None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))>\n(256, 120, 7)\n(256, 1) (256, 120, 7)\n\n\n\nimport json\nfor w in (model.get_weights()):\n    print(w.shape)\n\njson.loads(model.to_json())['config']['layers']\n\n(7, 128)\n(32, 128)\n(128,)\n(32, 1)\n(1,)\n\n\n[{'class_name': 'InputLayer',\n  'config': {'batch_input_shape': [None, 120, 7],\n   'dtype': 'float32',\n   'sparse': False,\n   'ragged': False,\n   'name': 'input_1'},\n  'name': 'input_1',\n  'inbound_nodes': []},\n {'class_name': 'LSTM',\n  'config': {'name': 'lstm',\n   'trainable': True,\n   'dtype': 'float32',\n   'return_sequences': False,\n   'return_state': False,\n   'go_backwards': False,\n   'stateful': False,\n   'unroll': False,\n   'time_major': False,\n   'units': 32,\n   'activation': 'tanh',\n   'recurrent_activation': 'sigmoid',\n   'use_bias': True,\n   'kernel_initializer': {'class_name': 'GlorotUniform',\n    'config': {'seed': None},\n    'shared_object_id': 1},\n   'recurrent_initializer': {'class_name': 'Orthogonal',\n    'config': {'gain': 1.0, 'seed': None},\n    'shared_object_id': 2},\n   'bias_initializer': {'class_name': 'Zeros',\n    'config': {},\n    'shared_object_id': 3},\n   'unit_forget_bias': True,\n   'kernel_regularizer': None,\n   'recurrent_regularizer': None,\n   'bias_regularizer': None,\n   'activity_regularizer': None,\n   'kernel_constraint': None,\n   'recurrent_constraint': None,\n   'bias_constraint': None,\n   'dropout': 0.0,\n   'recurrent_dropout': 0.0,\n   'implementation': 2},\n  'name': 'lstm',\n  'inbound_nodes': [[['input_1', 0, 0, {}]]]},\n {'class_name': 'Dense',\n  'config': {'name': 'dense',\n   'trainable': True,\n   'dtype': 'float32',\n   'units': 1,\n   'activation': 'linear',\n   'use_bias': True,\n   'kernel_initializer': {'class_name': 'GlorotUniform',\n    'config': {'seed': None}},\n   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n   'kernel_regularizer': None,\n   'bias_regularizer': None,\n   'activity_regularizer': None,\n   'kernel_constraint': None,\n   'bias_constraint': None},\n  'name': 'dense',\n  'inbound_nodes': [[['lstm', 0, 0, {}]]]}]"
  },
  {
    "objectID": "about.html#partial-derivatives-of-fco2-1",
    "href": "about.html#partial-derivatives-of-fco2-1",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Partial Derivatives of FCO2",
    "text": "Partial Derivatives of FCO2"
  },
  {
    "objectID": "about.html#partial-derivatives-of-fch4",
    "href": "about.html#partial-derivatives-of-fch4",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Partial Derivatives of FCH4",
    "text": "Partial Derivatives of FCH4"
  },
  {
    "objectID": "about.html#objective",
    "href": "about.html#objective",
    "title": "Tools and Guidance for Applying Neural Networks to Eddy Covariance Data",
    "section": "Objective",
    "text": "Objective\nProvide a framework for applying NN models to EC data for descriptive analysis and inferential modelling.\n\nThe github repository linked here has functional examples that can be used to setup NN models.\n\nRuns in Python and Tensorflow\n\nGPU support not required\n\nBut will decrease processing times"
  }
]