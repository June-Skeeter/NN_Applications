---
title: "A Framework for Applying Neural Networks to Eddy Covariance Data"
format:  pdf
jupyter: python3
echo: false
author:
  - name: Dr. June Skeeter
    orcid: 0000-0002-7051-343X
    email: june.skeeter@ubc.ca
    url: https://github.com/June-Skeeter
    affiliations:
    - ref: UBC
  - name: Dr. Sara Knox
    affiliations:
    - ref: UBC
affiliations:
    - id: UBC
      name: University of British Columbia
      department: Department of Geography
      address: 1984 West Mall
      city: Vancouver
      country: CA
      postal-code: V6T 1Z2
keywords: [Neural Networks, Eddy Covariance, Carbon Budgets]
bibliography: ML_EC.bib
csl: american-geophysical-union.csl
---

### Target Journals 

Ag & Forest Met
EGU Bio geosciences
AGU Bio geosciences

# Abstract

Eddy covariance (EC) is a passive, non-invasive method for measuring ecosystem-atmosphere trace gas exchange.  It has become increasingly popular in recent years as hardware and software have become more accessible.  Eddy covariance cannot measure fluxes continuously because the assumptions underpinning the method are not valid under all meteorologic conditions, but data from EC sites are widely used to monitor ecosystem scale energy, water, and carbon exchange.  Trace gas fluxes tend to exhibit spatially and temporally variable, non-linear dependence upon numerous drivers. Multi-year EC data sets have hundreds of thousands of data points and flux time series contain both noise and data gaps. These factors make EC data poorly suited for analysis with traditional statistical methods. Here we present a guidance for leveraging the flexibility and functionality of Neural network (NN) models for working with EC data.  Neural networks are a flexible machine learning method and an ideal tool for working with large multivariate datasets with complex non-linear dependencies.  They offer control over the structure a model and inspection of model derivatives provides a method for ensuring that relationships mapped by a NN are physically plausible.  We demonstrate methods for inferential modelling with NN and EC data, provide examples demonstrating how model derivatives can be used to detect and visualize the functional relationships, and offer comparisons to other common ML methods.

# Introduction 

Eddy covariance (EC) is a passive, non-invasive method for semi-continuous monitoring of ecosystem-atmosphere trace gas exchange.  The method has become increasingly in recent years as hardware and software have become more accessible.  Eddy covariance stations are now widespread across the globe; measure energy, water, and greenhouse gas fluxes from Arctic tundra to tropical rain forests.  The method is an integral component of understanding how ecosystems influence and respond to climate change. Raw EC data are collected at high frequency (10 - 20 Hz) and are typically processed to estimate mean half-hourly or hourly fluxes.  The processed EC data are then used to calculate ecosystem-scale carbon and water budgets and to better understand the dynamics governing ecosystem-scale trace gas exchange.


The assumptions underpinning the EC based flux estimation require specific meteorologic conditions; when these conditions do not hold EC data must be discarded (@burba_eddy_2022).  Between sub-optimal weather conditions and operational constraints (maintenance, power supply, etc.) it is not uncommon for over half of carbon dioxide (CO~2~) or methane (CH~4~) flux data to be missing in a given site-year (@falge_gap_2001; @delwiche_fluxnet-ch4_2021).  Missing data result in an observation bias; fair-weather, daytime conditions are over-represented in EC time series data.  This presents a challenge for flux response mapping, gap-filling, and upscaling; practitioners must select models capable of handling the bias.   Marginal distribution sampling (MDS), a method used by both FLUXNET and ICOS to gap fill EC data has recently been shown to induce considerable bias in Carbon (C) balance estimates at high latitude sites @vekuri_widely-used_2023.

Machine learning methods, particularly decision tree based approaches, such as random forests (RF) and eXtreme gradient boosting (XGBoost) have frequently been suggested as viable alternatives (e.g., @kim_gap-filling_2020; @irvin_gap-filling_2021; @vekuri_widely-used_2023).


A number of papers have been written about this issue, particularly in regards to gap filling (e.g., @moffat_comprehensive_2007; @dengel_testing_2013; @kim_gap-filling_2020; @irvin_gap-filling_2021; @lee_combining_2021; @gao_eddy_2023).  


EC data is poorly suited for traditional statistical methods @wegman_computational_1988



Site specific approaches are often needed work with EC data, but from these {site specific} functional relationships, we hope to glean {universal} truths.

We need to ensure that the results conform to our values reasonably expected {given our understanding of system/the framework of our conceptual model}.  And if they don't, we need to be able to see "why" in order to ensure ...


As the period of record at sites extend (e.g, decades)

We provide examples to demonstrate how NN models can be used for inferential modelling

detecting and mapping functional relationships 
pattern recognition and feature detection
demonstrate how they can be used to map response functions,
show their ability to inferential modelling Gap-filling
gap filling of EC data and for understanding the 

They offer the user more control over the structure of the model and inspection of the model derivatives provides a method for validating that the relationships mapped by a model are physically plausible.  


Site specific approaches are often needed work with EC data, but from these {site specific} functional relationships, we hope to glean {universal} truths.

We need to ensure that the results conform to our values reasonably expected {given our understanding of system/the framework of our conceptual model}.  And if they don't, we need to be able to see "why" in order to ensure ...

##	Machine Learning

Machine learning is a family of methods whereby an algorithm builds a predictive model from experience by learning patterns in datasets @nichols_machine_2018.  Algorithms can learn the functional relationships (and noise) in a dataset without them being explicitly programmed in advance @smith_neural_1993.  Care must be taken to prevent overfitting when using machine learning, but there are numerous methods available to account for this issue @sarle_stopped_1995; @khosravi_comprehensive_2011 @sarle_compaineural-nets_2014.  Machine learning methods are particularly useful for analyzing EC flux data because of the complex non-linear relationships governing uptake and emissions and dynamics which can vary widely between different sites @moffat_comprehensive_2007 @moffat_characterization_2010 @dengel_testing_2013 @kim_gap-filling_2020 @irvin_gap-filling_2021.

Below is a brief discussion of the pros and cons of two machine learning methods that are commonly applied to EC data.


## Random Forests


Recent studies have suggested that random forest (RF) models are an ideal choice for gap filling EC data because: they are easy to implement, they can be trained quickly, and they yield very high R2 scores without overfitting the training data.  However, RF models have key limitations that may limit the applicability gap filling.  They are incapable of extrapolation and they offer little insight into the functional relationships being mapped by the models.

Random Forest (RF) regression is a powerful machine learning method that is computationally inexpensive so models can be applied with relative ease.  They were first introduced by @breiman_random_2001; building on the Classification and Regression Tree (CART) model introduced by @breiman_classification_1984.  The RF method involves training a large number (e.g., n ≥ 100) of CART models on bootstrapped iterations of a dataset @breiman_random_2001.  Individual CART models are prone to overfitting, particularly in high dimensional feature space.  A small change in input values can yield large, unpredictable changes in output values.  @breiman_random_2001 show that the Law of Large Numbers prevents RF models from overfitting.

Two recent studies (@kim_gap-filling_2020 and @irvin_gap-filling_2021) have suggested RF models are ideal for gap filling complex EC data sets because they are fast, easy to use, and produce high r2 values in artificial gap filling experiments.

We caution against the use of RF models because they have two key limitation: 

  1) RF models are incapable of extrapolation; they cannot make projections outside the domain they were trained on and often perform worse than simple linear regression for projection problems @hengl_random_2018.  Gaps in EC data are frequently due to conditions in which the method is incapable of accurately measuring fluxes (e.g. precipitation events, low friction velocity, etc.).  These gaps represent a systematic bias in flux observations and using a RF model to predict fluxes during these gaps may not be the best approach.  Artificial gap filling experiments do not adequately address this issue because the gaps are draw from within the domain of valid EC observations, which are themselves systematically biased.  To address the systematic bias, an inferential statistical method would be preferable.

  2) RF models do not yield response functions that can be inspected for physical plausibility.  CART models fit training data by minimizing mean squared error through a series of binary splits, producing a complex stepwise function.  The RF model averages a number of CART models to *emulate* a continuous function; but this only produces a more complex stepwise function.  Further, interpreting RF models is difficult.  We can calculate relative importance of input features, and view the "classification boundaries" in low dimensional (n<=2) feature space.  WE can also inspect individual trees within the RF, but the hundreds of trees in a RF model cannot be visualized in aggregate and it is essentially impossible to follow the flow of information through a RF model (see below).  

## Neural Networks

Neural networks are a flexible machine learning method and an ideal tool for working with large multivariate datasets with complex non-linear dependencies.  They offer control over the structure a model and inspection of model derivatives provides a method for ensuring that relationships mapped by a NN are physically plausible.  We demonstrate methods for inferential modelling with NN and EC data, provide examples demonstrating how model derivatives can be used to detect and visualize the functional relationships, and offer comparisons to other common ML methods.

Neural networks (NN) are an incredibly flexible and powerful group of machine learning algorithms.  They are ideally suited for handling data with complex non-linear relationships because they make no prior assumptions about the distribution of a dataset @hornik_approximation_1991; @melesse_artificial_2005 @khosravi_comprehensive_2011.  They are frequently used for feature detection and pattern recognition for tasks like speech recognition and computer vision.  They are much more computationally expensive than RF models and have historically been considered impractical because of the time required to train models and fine-tune hyper parameters.  Recent advancements in hardware (e.g., graphics processing units) and software (e.g., TensorFlow) have streamlined the NN training process.

A key advantage of NN models over RF models is they are capable of extrapolation.  The continuous, non-linear response functions NNs produce can be used to estimate outputs for unseen inputs.  This is useful for gap filling, but this point is not addressed in the artificial gap filling experiments advocating for RF models (e.g., @kim_gap-filling_2020; @irvin_gap-filling_2021).  Because of their flexibility, NN are widely used for gap filling EC observations of CO2 fluxes (e.g., @papale_new_2003; @moffat_comprehensive_2007; @schafer_carbon_2019; @skeeter_vegetation_2020; @lee_combining_2021; @skeeter_controls_2022
) and CH4 fluxes (e.g., @dengel_testing_2013; @knox_fluxnet-ch4_2019; @skeeter_vegetation_2020; @skeeter_controls_2022).  Their predictive capacity also makes them useful for upscaling; they have been used for spatial upscaling (e.g., @eshel_listening_2019) and temporal upscaling (e.g. @melesse_artificial_2005; @dou_estimating_2018).  Additionally, NN have been successfully used to partition NEE into GPP and ER (@desai_cross-site_2008; @tramontana_partitioning_2020) and they have been applied to identify CH4 flux drivers at wetland sites (@knox_biophysical_2016; @skeeter_controls_2022).

A commonly cited downside of NN algorithms is that trained models are “black boxes”.  That is, they can be used to produce a final result but we cannot discern any information about the importance of input features or the functional relationships in the model (@melesse_artificial_2005; @moffat_comprehensive_2007; @irvin_gap-filling_2021).  However, this is not true.  Inspecting model derivatives provides a way to open up the “black box”.  Calculating the partial first (and second) derivatives of every input parameter (see below).  This can be used to evaluate the relative influence of inputs, prune inputs from high dimensional models, and visualize the functional relationships identified to ensure they are physiologically plausible @dimopoulos_use_1995; @gevrey_review_2003; @amiri_most_2020.  These methods have only been applied to a limited number of EC studies (e.g., @moffat_characterization_2010; @skeeter_controls_2022).  Here we present a framework for formally incorporating this approach to EC studies.

## Objectives

Provide a framework that can be used to apply NN methods to EC data for:

* Flux-response mapping
* Gap-filling
* Temporo-spatial upscaling


# The Data

test @fig-polar

```{python}
#| label: Some Data
#| fig-cap: "A line plot on a polar axis :D"

import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 4 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```


  
### References

::: {#refs}
:::
