{"title":"A Framework for Applying Neural Networks to Eddy Covariance Data","markdown":{"yaml":{"title":"A Framework for Applying Neural Networks to Eddy Covariance Data","jupyter":"python3","echo":false,"fig-dpi":300,"format":{"revealjs":{"code-fold":true,"controls":true,"navigation-mode":"linear","controls-layout":"bottom-right","controls-tutorial":true,"slide-number":true,"show-slide-number":"all","pdfMaxPagesPerSlide":1}},"author":[{"name":"Dr. June Skeeter","email":"june.skeeter@ubc.ca","url":"https://github.com/June-Skeeter","affiliations":[{"ref":"UBC"}]}],"affiliations":[{"id":"UBC","name":"University British Columbia","department":"Department Geography","address":"1984 West Mall","city":"Vancouver, BC, Canada","postal-code":"V6T 1Z2"}],"keywords":["Eddy Covariance","Micrometeorology","Neural Networks","Modelling"]},"headingText":"Eddy Covariance","containsRefs":false,"markdown":"\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nEcosystem-scale fluxes energy, water, and trace gases.\n\n* Voluminous data sets\n  * Noisy & gap-prone\n* Ideally suited for machine learning!\n\n:::\n\n::: {.column width=\"50%\"}\n\n<img src=\"images/BB1_System.jpg\" alt=\"your-image-description\" style=\"border: 2px solid  black;\">\n\n<h3>Burns Bog EC Station<br>Delta, BC</h3>\n\n:::\n\n::::\n\n\n## Neural Networks\n\n**Universal approximators**: they can map any continuous function to an arbitrary degree accuracy.\n\n* With sufficient hidden nodes, they will fit **any** pattern in a dataset\n  * Care must be taken to ensure the patterns are real\n    * Have often been treated as \"black boxes\"\n\n* Well suited for non-linear, multi-variate response functions\n  * Capable *interpolation* and **extrapolation**\n\n## Commonly Cited Limitations\n\n\n```{python}\n#| tbl-colwidths: [40,60]\nimport pandas as pd\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ndf = pd.read_csv('About.csv',sep='|',index_col='Drawback')\nMarkdown(tabulate(\n  df, \n  headers=[\"Issue\", \"Solutions\"]\n))\n\n```\n\n# Objective\n\nProvide a framework for applying NN models can be applied to EC data for both descriptive analysis and inferential modelling. \n \n* The [github repository](https://github.com/June-Skeeter/NN_Applications) linked to this presentation has functional examples that can be used to apply NN models.\n  * Runs in Python and Tensorflow: *GPU support not required*\n\n\n## Example Data\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\nBB1 EC station\n\n* Beakrush-Sphagnum ecosystem undergoing active restoration\n* 8+ years of CO<sub>2</sub> and CH<sub>4</sub> flux observations\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n<img src=\"images/BB1.jpg\" alt=\"your-image-description\" style=\"border: 2px solid  black; width: 100%\">\n\n\n:::\n::::\n\n\n## Training Procedures\n\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n\nAn iterative process:\n\n* Three way cross-validation\n  * Train & validate - random split by model \n  * Test - consistent between models\n\n* Ensemble-size\n  * Larger ensemble > more robust model\n  * N <= 10 likely suitable for data exploration\n    * N >=30 would give best performance\n\n:::\n\n::: {.column width=\"30%\"}\n\n<img src=\"images/NN_Workflows.png\" alt=\"your-image-description\">\n\n\n:::\n\n::::\n\n## How to Prune a Model\n\nCalculate partial first derivative of the output with respect to each input over test data domain.\n\n* **Relative Influence (RI)**:  Normalized Sum of squared derivatives (SSD)\n\n* Iteratively prune inputs with RI below a reference threshold\n  * Random input assess base-level performance\n\n* Train final model without random scalar\n\n## Pruning a FCO<sub>2</sub> Model\n\n```{python}\n#| label: RI of models\n#| layout-ncol: 1\n#| warning: False\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom Scripts import MiscFuncs,PlotHelpers\n\nfig,ax=plt.subplots(1,2,sharey=True,sharex=True)\n\nBase = 'Test'\nName = 'Full_Model'\nRI = pd.read_csv(f'Models/{Base}/{Name}/model_RI.csv',index_col=[0])\nRI = RI.sort_values(by=f'RI_bar',ascending=True)\nPlotHelpers.makeRI_plot(ax[0],RI,Title='Over-Parametrized Model')\n\nName = 'Final_Model'\nRI = pd.read_csv(f'Models/{Base}/{Name}/model_RI.csv',index_col=[0])\nRI = RI.sort_values(by=f'RI_bar',ascending=True)\nPlotHelpers.makeRI_plot(ax[1],RI,Title='Pruned Model')\nax[1].set_ylabel('')\nplt.tight_layout()\n\n```\n\n## Model Inspection\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nPlot the model outputs using the test data.\n\n\n::: {style=\"font-size: 80%;\"}\n\n```{python}\n#| label: Validation\n#| layout-ncol: 1\n#| warning: False\n#| tbl-colwidths: [25,75]\n\nimport numpy as np\nfrom sklearn import metrics\n\ndf = pd.read_csv(f'Models/{Base}/{Name}/model_output.csv',index_col=[0])\n\nunit = '$\\mu mol$ $m^{-2}s^{-1}$'\n\nx,y='target','y_bar'\nr2 = str(np.round(metrics.r2_score(df[x],df[y]),2))\nRMSE = str(np.round(metrics.mean_squared_error(df[x],df[y])**.5,2))+f' {unit}'\n\nm = pd.DataFrame(index=['RMSE','r<sup>2</sup>'],data={'Metrics':[RMSE,r2]})\nMarkdown(tabulate(\n  m, \n  headers=[\"Metric\", \"Score\"]\n))\n\n\n```\n\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n```{python}\n#| label: Performance of final model\n#| layout-ncol: 1\n#| warning: False\n\ndf = pd.read_csv(f'Models/{Base}/{Name}/model_output.csv',index_col=[0])\n\nunit = '$\\mu mol m^{-2} s^{-1}$'\n\nfig,ax=plt.subplots(figsize=(5,5))\nax = PlotHelpers.make1_1_Plot(ax,df,'target','y_bar',unit=unit)\nax.set_ylabel('NN Estimate')\nax.set_xlabel('EC Observation')\nax.set_title('')\nradom_code_to_mask_text=0\n\n```\n\n\n:::\n\n::::\n\n## Plotting Derivatives\n    \nHelp ensure mapped relationships are physically plausible\n\n* An **essential step** and **key advantage** NN models\n* Raw derivatives show true feature responses\n  * 95% confidence intervals indicate model confidence in relationships\n* Normalized derivatives scaled by input variance\n  * View relative on common scale\n\n\n## Partial Derivatives of FCO<sub>2</sub>\n \n```{python}\n#| label: Derivatives of final model\n#| layout-ncol: 1\n#| warning: False\n\nMax=4\nTop = RI.sort_values(by=f'RI_bar',ascending=False).index[:Max]\n\ncols = 2\nnpi=len(RI.index)\nrows = int(np.ceil(len(Top)/2))\n\nfig,axes=plt.subplots(rows,cols,sharey=True)\n\naxes = axes.flatten()\n\nmod = ''\n\nfor i,xi in enumerate(Top):\n    df_int = MiscFuncs.byInterval(df,f'{xi}',[f'dy_d{xi}{mod}'],bins=50)\n    PlotHelpers.CI_Plot(axes[i],df_int,f'dy_d{xi}{mod}')\nplt.tight_layout()\n\naxes[i].get_ylabel()\n\nfor ax in axes:\n    l = ax.get_ylabel()\n    ax.set_ylabel(l.split('_norm')[0].replace('_',' / ').replace('y','FCO2'))\n\n\nradom_code_to_mask_text=0\n\n```\n\n## Normalized Partial Derivatives\n \n```{python}\n#| label: Normalized Derivatives of final model\n#| layout-ncol: 1\n#| warning: False\n\nimport numpy as np\n\nMax=4\nTop = RI.sort_values(by=f'RI_bar',ascending=False).index[:Max]\n\ncols = 2\nnpi=len(RI.index)\nrows = int(np.ceil(len(Top)/2))\n\nfig,axes=plt.subplots(rows,cols,sharey=True)\n\naxes = axes.flatten()\n\nmod = '_norm'\n\nfor i,xi in enumerate(Top):\n    df_int = MiscFuncs.byInterval(df,f'{xi}',[f'dy_d{xi}{mod}'],bins=50)\n    PlotHelpers.CI_Plot(axes[i],df_int,f'dy_d{xi}{mod}')\nplt.tight_layout()\n\naxes[i].get_ylabel()\n\nfor ax in axes:\n    l = ax.get_ylabel()\n    ax.set_ylabel(l.split('_norm')[0].replace('_',' / ').replace('y',' '))\n\n\nradom_code_to_mask_text=0\n\n```\n\n\n## Next Steps\n\n* Custom NN architecture: Separating input layers may allow us partition fluxes.\n  * FCO<sub>2</sub> into GPP and ER\n  * FCH<sub>4</sub> into methanogenesis, methanotrophy, transport\n* Flux footprint analysis: Models can help account for spatial heterogeneity within a a footprint\n  * NN could also be trained to calculate/classify footprints!\n* u* filtering: Partial derivatives of u* could give thresholds for filtering\n\n# Thank You\n\nQuestions?\n\n## Why not use a Random Forest?\n\nRF models are great! ... for classifying discrete objects\n\n* But, it's my view that applying them to continuous data is misguided\n* They are [poorly suited](https://github.com/June-Skeeter/NN_Applications) for interpolation and incapable of extrapolation"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"about.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"title":"A Framework for Applying Neural Networks to Eddy Covariance Data","jupyter":"python3","author":[{"name":"Dr. June Skeeter","email":"june.skeeter@ubc.ca","url":"https://github.com/June-Skeeter","affiliations":[{"ref":"UBC"}]}],"affiliations":[{"id":"UBC","name":"University British Columbia","department":"Department Geography","address":"1984 West Mall","city":"Vancouver, BC, Canada","postal-code":"V6T 1Z2"}],"keywords":["Eddy Covariance","Micrometeorology","Neural Networks","Modelling"],"controls":true,"navigationMode":"linear","controlsLayout":"bottom-right","controlsTutorial":true,"slideNumber":true,"showSlideNumber":"all","pdfMaxPagesPerSlide":1}}}}