---
title: "A Framework for Applying Neural Networks to Eddy Covariance Data"
# format:
#   html:
jupyter: python3
format:
  revealjs:
    code-fold: true
    controls: true
    navigation-mode: linear
    controls-layout: bottom-right
    controls-tutorial: true
    # margin: 0.05
    # width: 1200
    # css: style.css
    slide-number: true
    show-slide-number: all
    pdfMaxPagesPerSlide: 1
  # pdf:
  #   code-fold: true
  
author:
  - name: Dr. June Skeeter
    # orcid: 0000-0002-7051-343X
    email: june.skeeter@ubc.ca
    url: https://github.com/June-Skeeter
    affiliations:
    - ref: UBC
#   - name: Dr. Sara Knox
#     email: sara.knox@ubc.ca
#     affiliations:
#     - ref: UBC
affiliations:
    - id: UBC
      name: University of British Columbia
      department: Department of Geography
      address: 1984 West Mall
      city: Vancouver, BC, Canada
      postal-code: V6T 1Z2
keywords: [Eddy Covariance, Micrometeorology, Neural Networks, Modelling]
citeproc: True
bibliography: ML_EC.bib

---

## Introduction

Neural networks (NN) are flexible, powerful machine learing algorithms.

* **Universal approximators**: can map any continuous function to an arbitrary degree fo accuracy.
    * Given sufficient "hidden nodes", will fit **any** pattern in a dataset
        * Care must be taken to ensure the paterns are real

## Introduction

:::: {.columns}

::: {.column width="40%"}

### Pros

* Well suited for non-linear, multi-variate response functions
* Robust to over-fitting

:::

::: {.column width="60%"}

### Cons

* Computationally expensive
* Complex and difficult to implement
* "Black Boxes"

:::

::::

## Introduction

Eddy Covariance (EC) measures spatially integrated, semi-continouous, ecosystem-scale fluxes of energy, water, and trace gas.

* Noisy, voluminous EC data sets are poorly suited for traditional statistical methods @wegman_computational_1988
    * Site to site varriability makes standard ana approaches are often needed

* We need to ensure our results are reasonable given our understanding of the system.
    * And if they don't, we need to be able to investigate **why**
  
* As the period of record at flux sites extend (e.g, decades) the task only becomes more complex
    * A sliver lining: voluminous data sets are ideally suited for computational statistics!

# Objectives

Provide a tool set with functional examples demonstrating how NN models can be applied to EC data for both descriptive analysis and inferential modelling.
 
* The [github repository](https://github.com/June-Skeeter/NN_Applications) linked to this presentation has Python code that can be used to apply NN models.


## A Simple 2D Example Calculating and Visualizing VPD

The Vapor Pressure Deficit (VPD) decreases exponentially as a function of air temperature (Ta) and linearly as a function of relative humidity (RH).  We can calculate VPD (kPa) from Ta in ($\circ$ C) and RH (%) as follows:

$$ ea_H = 0.61365*np.exp((17.502*Ta)/(240.97+Ta))$$
$$e_H = RH*ea_H/100$$
$$VPD = (ea_H - e_H)*10$$

```{python}
#| label: Estimating VPD
#| fig-cap: "This plot shows the relationship between VPD, TA, and RH over a range of possible values"
#| layout-ncol: 1
#| warning: False

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from Scripts import MiscFuncs,PlotHelpers

units = {
    'TA':'$T_a^\circ$C',
    'RH':'RH %',
    'VPD':'VPD hPa'
    }

labels = {
    'TA':'Air Temperature',
    'RH':'Relative Humidity',
    'VPD':'Vapor Pressure Deficit'
    }

range_TA_RH,grid_TA,grid_RH,grid_VPD = MiscFuncs.Create_Grid(
    np.linspace(-50,50),# Define a TA range (in C)
    np.linspace(0,100), # Possible RH values
    MiscFuncs.Calc_VPD # Return Vapor Pressure Defecit
    )
    
bins = np.arange(-10,grid_VPD.max(),15)
cmap = 'PuRd'
norm = [0,grid_VPD.max()]

fig,ax=plt.subplots(1,figsize=(5,5))
PlotHelpers.Contour_Plot(fig,ax,grid_TA,grid_RH,grid_VPD,cmap=cmap,norm=norm,unit = units['VPD'],bins=bins)
ax.set_xlabel('Air Temperature $^\circ$C')
ax.set_ylabel('Relative Humidity %')
ax.set_title('Vapor Pressure Deficit (VPD)')
plt.tight_layout()

# Use tensorfolow to calculate the first partial derivative of the function
X_tensor = tf.convert_to_tensor(range_TA_RH.T)
with tf.GradientTape(persistent=True) as tape:
    tape.watch(X_tensor)
    VPD_est = MiscFuncs.Calc_VPD(X_tensor) 
# Get gradients of VPD_est with respect to X_tensor
Deriv = tape.gradient(VPD_est,X_tensor).numpy()


Derivatives = pd.DataFrame(
    data={
    'TA':range_TA_RH.T[:,0],
    'RH':range_TA_RH.T[:,1],
    'dVPD/dTA':Deriv[:,0],
    'dVPD/dRH':Deriv[:,1]
    }
)

fig,axes=plt.subplots(2,2,figsize=(8,8),sharey='row')

grid_dVPD_dTA = Deriv[:,0].T.reshape(grid_TA.shape)
grid_dVPD_dRH = Deriv[:,1].T.reshape(grid_RH.shape)

d_bins = np.arange(
    np.floor(Deriv).min(),np.ceil(Deriv).max(),.5
    )
d_cmap = 'bwr'
d_norm = [
    Deriv.min(),0, Deriv.max()
    ]
    
ax,_ = PlotHelpers.Contour_Plot(fig,axes[0,0],grid_TA,grid_RH,grid_dVPD_dTA,cmap = d_cmap,norm=d_norm,bins=d_bins)
ax.set_title('dVPD dTa')

ax,_ = PlotHelpers.Contour_Plot(fig,axes[0,1],grid_TA,grid_RH,grid_dVPD_dRH,cmap = d_cmap,norm=d_norm,bins=d_bins)
ax.set_title('dVPD dRH')


y=['dVPD/dTA']
df = MiscFuncs.byInterval(Derivatives,'TA',y,bins=100)
ax = PlotHelpers.CI_Plot(axes[1,0],df,y[0])
# ax.set_title('Partial First Derivative\nVPD with respect to Ta')

y=['dVPD/dRH']
df = MiscFuncs.byInterval(Derivatives,'RH',y,bins=100)
ax = PlotHelpers.CI_Plot(axes[1,1],df,y[0])
# ax.set_title('Partial First Derivative\nVPD with respect to RH')


plt.tight_layout()

```

## Partial Derivatives

```{python}
#| label: Derivatives of VPD
#| fig-cap: "This plot shows the partial first derivatives of VPD"
#| layout-ncol: 1
#| warning: False
grid_VPD.min()


```


## Example Data

BB1 Flux tower was established in 2015.

```{python}
from Scripts import ReadDB

# dbNames = {
#     'TA_1_1_1':'TA',
#     'RH_1_1_1':'RH'
# }

# read_new = False
# if read_new == False:
#     Data = ReadDB.get_Traces('BB',['TA_1_1_1','RH_1_1_1'],Dir='/mnt/c/Users/User/PostDoc_Work/database/')
#     print(Data)
#     Data = Data.dropna(axis=0)
#     Data = Data.rename(columns=dbNames)
#     Data.to_csv('temp/BB1_VPD.csv')

# else:
Site = 'BB'
Data = pd.read_csv(f'temp/{Site}_VPD.csv',parse_dates=['TimeStamp'],index_col='TimeStamp')
    
print(Data.head())

Data['VPD'] = MiscFuncs.Calc_VPD(Data['TA'],Data['RH'])
    
fig,axes=plt.subplots(1,3,figsize=(7,4))
Data.hist(column='TA',ax=axes[0],bins=20,edgecolor='k')
axes[0].set_xlabel(units['TA'])

Data.hist(column='RH',ax=axes[1],bins=20,edgecolor='k')
axes[1].set_xlabel(units['RH'])

Data.hist(column='VPD',ax=axes[2],bins=20,edgecolor='k')
axes[2].set_xlabel(units['VPD'])

plt.tight_layout()

Data.describe().round(1)
```


## Artificial Gaps


```{python}
#| label: fig-format
#| fig-cap: "Format the training data" 
#| layout-ncol: 1
#| warning: False

X_vars = ['TA', 'RH']
Y_var = 'VPD'

# X_full = Data[X_vars].values
# Y_full = Data[Y_var].values
# print('Full Training Samples: ',Y_full.shape)

# Mask = np.array([-1,1])
# # Masked dataset for training
# x_mask = Data.loc[(
#     (Data[Y_var]<Mask.min())|(Data[Y_var]>Mask.max())
#     ),X_vars].values
# y_mask = Data.loc[(
#     (Data[Y_var]<Mask.min())|(Data[Y_var]>Mask.max())
#     ),Y_var].values
# print('Masked Samples: ',y_mask.shape)

# # Missing values for assessing performance
# x_missing = Data.loc[Data[Y_var].isin(y_mask)==False,X_vars].values
# y_missing = Data.loc[Data[Y_var].isin(y_mask)==False,Y_var].values
# print('Missing Samples: ',y_missing.shape)

# Make some artificial gap scenarios

# Scenarios = {
#     "Random Drop Out":{}
#     "Missing Middle":{}
#     "Clipped Wings":{}
# }

Mask = np.array([[-1,1],[2,3]])
Masked,Dropped = MiscFuncs.makeMask(Data,Y_var,Mask)

print(Masked.shape,Dropped.shape)


# print(Mask.ndim)
for mask in Mask:
    print(mask)
```



## Conclusions

They offer the user more control over the structure of the model and inspection of the model derivatives provides a method for validating that the relationships mapped by a model are physically plausible.

## Questions