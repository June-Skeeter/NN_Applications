---
title: "A Framework for Applying Neural Networks to Eddy Covariance Data"
# format:
#   html:
jupyter: python3
echo: false
fig-dpi: 300
format:
  revealjs:
    code-fold: true
    controls: true
    navigation-mode: linear
    controls-layout: bottom-right
    controls-tutorial: true
    # margin: 0.05
    # width: 1200
    # css: style.css
    slide-number: true
    show-slide-number: all
    pdfMaxPagesPerSlide: 1
  # pdf:
  #   code-fold: true
  
author:
  - name: Dr. June Skeeter
    # orcid: 0000-0002-7051-343X
    email: june.skeeter@ubc.ca
    url: https://github.com/June-Skeeter
    affiliations:
    - ref: UBC
#   - name: Dr. Sara Knox
#     email: sara.knox@ubc.ca
#     affiliations:
#     - ref: UBC
affiliations:
    - id: UBC
      name: University British Columbia
      department: Department Geography
      address: 1984 West Mall
      city: Vancouver, BC, Canada
      postal-code: V6T 1Z2
keywords: [Eddy Covariance, Micrometeorology, Neural Networks, Modelling]

---

## Eddy Covariance


:::: {.columns}

::: {.column width="50%"}

Ecosystem-scale fluxes energy, water, and trace gases.

* Voluminous data sets
  * Noisy & gap-prone
* Ideally suited for machine learning!

:::

::: {.column width="50%"}

<img src="images/BB1_System.jpg" alt="your-image-description" style="border: 2px solid  black;">

<h3>Burns Bog EC Station<br>Delta, BC</h3>

:::

::::


## Neural Networks

**Universal approximators**: they can map any continuous function to an arbitrary degree accuracy.

* With sufficient hidden nodes, they will fit **any** pattern in a dataset
  * Care must be taken to ensure the patterns are real
    * Have often been treated as "black boxes"

* Well suited for non-linear, multi-variate response functions
  * Capable *interpolation* and **extrapolation**

## Commonly Cited Limitations


```{python}
#| tbl-colwidths: [40,60]
import pandas as pd

from IPython.display import Markdown
from tabulate import tabulate

df = pd.read_csv('About.csv',sep='|',index_col='Drawback')
Markdown(tabulate(
  df, 
  headers=["Issue", "Solutions"]
))

```

# Objective

Provide a framework for applying NN models can be applied to EC data for both descriptive analysis and inferential modelling. 
 
* The [github repository](https://github.com/June-Skeeter/NN_Applications) linked to this presentation has functional examples that can be used to apply NN models.
  * Runs in Python and Tensorflow: *GPU support not required*


## Example Data

:::: {.columns}

::: {.column width="40%"}

BB1 EC station

* Beakrush-Sphagnum ecosystem undergoing active restoration
* 8+ years of CO<sub>2</sub> and CH<sub>4</sub> flux observations

:::

::: {.column width="60%"}


<img src="images/BB1.jpg" alt="your-image-description" style="border: 2px solid  black; width: 100%">


:::
::::


## Training Procedures


:::: {.columns}

::: {.column width="70%"}

An iterative process:

* Three way cross-validation
  * Train & validate - random split by model 
  * Test - consistent between models

* Ensemble-size
  * Larger ensemble > more robust model
  * N <= 10 likely suitable for data exploration
    * N >=30 would give best performance

:::

::: {.column width="30%"}

<img src="images/NN_Workflows.png" alt="your-image-description">


:::

::::

## How to Prune a Model

Calculate partial first derivative of the output with respect to each input over test data domain.

* **Relative Influence (RI)**:  Normalized Sum of squared derivatives (SSD)

* Iteratively prune inputs with RI below a reference threshold
  * Random input assess base-level performance

* Train final model without random scalar

## Pruning a FCO<sub>2</sub> Model

```{python}
#| label: RI of models
#| layout-ncol: 1
#| warning: False
import pandas as pd
import matplotlib.pyplot as plt
from Scripts import MiscFuncs,PlotHelpers

fig,ax=plt.subplots(1,2,sharey=True,sharex=True)

Base = 'Test'
Name = 'Full_Model'
RI = pd.read_csv(f'Models/{Base}/{Name}/model_RI.csv',index_col=[0])
RI = RI.sort_values(by=f'RI_bar',ascending=True)
PlotHelpers.makeRI_plot(ax[0],RI,Title='Over-Parametrized Model')

Name = 'Final_Model'
RI = pd.read_csv(f'Models/{Base}/{Name}/model_RI.csv',index_col=[0])
RI = RI.sort_values(by=f'RI_bar',ascending=True)
PlotHelpers.makeRI_plot(ax[1],RI,Title='Pruned Model')
ax[1].set_ylabel('')
plt.tight_layout()

```

## Model Inspection


:::: {.columns}

::: {.column width="50%"}

Plot the model outputs using the test data.


::: {style="font-size: 80%;"}

```{python}
#| label: Validation
#| layout-ncol: 1
#| warning: False
#| tbl-colwidths: [25,75]

import numpy as np
from sklearn import metrics

df = pd.read_csv(f'Models/{Base}/{Name}/model_output.csv',index_col=[0])

unit = '$\mu mol$ $m^{-2}s^{-1}$'

x,y='target','y_bar'
r2 = str(np.round(metrics.r2_score(df[x],df[y]),2))
RMSE = str(np.round(metrics.mean_squared_error(df[x],df[y])**.5,2))+f' {unit}'

m = pd.DataFrame(index=['RMSE','r<sup>2</sup>'],data={'Metrics':[RMSE,r2]})
Markdown(tabulate(
  m, 
  headers=["Metric", "Score"]
))


```

:::

:::

::: {.column width="50%"}


```{python}
#| label: Performance of final model
#| layout-ncol: 1
#| warning: False

df = pd.read_csv(f'Models/{Base}/{Name}/model_output.csv',index_col=[0])

unit = '$\mu mol m^{-2} s^{-1}$'

fig,ax=plt.subplots(figsize=(5,5))
ax = PlotHelpers.make1_1_Plot(ax,df,'target','y_bar',unit=unit)
ax.set_ylabel('NN Estimate')
ax.set_xlabel('EC Observation')
ax.set_title('')
radom_code_to_mask_text=0

```


:::

::::

## Plotting Derivatives
    
Help ensure mapped relationships are physically plausible

* An **essential step** and **key advantage** NN models
* Raw derivatives show true feature responses
  * 95% confidence intervals indicate model confidence in relationships
* Normalized derivatives scaled by input variance
  * View relative on common scale


## Partial Derivatives of FCO<sub>2</sub>
 
```{python}
#| label: Derivatives of final model
#| layout-ncol: 1
#| warning: False

Max=4
Top = RI.sort_values(by=f'RI_bar',ascending=False).index[:Max]

cols = 2
npi=len(RI.index)
rows = int(np.ceil(len(Top)/2))

fig,axes=plt.subplots(rows,cols,sharey=True)

axes = axes.flatten()

mod = ''

for i,xi in enumerate(Top):
    df_int = MiscFuncs.byInterval(df,f'{xi}',[f'dy_d{xi}{mod}'],bins=50)
    PlotHelpers.CI_Plot(axes[i],df_int,f'dy_d{xi}{mod}')
plt.tight_layout()

axes[i].get_ylabel()

for ax in axes:
    l = ax.get_ylabel()
    ax.set_ylabel(l.split('_norm')[0].replace('_',' / ').replace('y','FCO2'))


radom_code_to_mask_text=0

```

## Normalized Partial Derivatives
 
```{python}
#| label: Normalized Derivatives of final model
#| layout-ncol: 1
#| warning: False

import numpy as np

Max=4
Top = RI.sort_values(by=f'RI_bar',ascending=False).index[:Max]

cols = 2
npi=len(RI.index)
rows = int(np.ceil(len(Top)/2))

fig,axes=plt.subplots(rows,cols,sharey=True)

axes = axes.flatten()

mod = '_norm'

for i,xi in enumerate(Top):
    df_int = MiscFuncs.byInterval(df,f'{xi}',[f'dy_d{xi}{mod}'],bins=50)
    PlotHelpers.CI_Plot(axes[i],df_int,f'dy_d{xi}{mod}')
plt.tight_layout()

axes[i].get_ylabel()

for ax in axes:
    l = ax.get_ylabel()
    ax.set_ylabel(l.split('_norm')[0].replace('_',' / ').replace('y',' '))


radom_code_to_mask_text=0

```


## Next Steps

* Custom NN architecture: Separating input layers may allow us partition fluxes.
  * FCO<sub>2</sub> into GPP and ER
  * FCH<sub>4</sub> into methanogenesis, methanotrophy, transport
* Flux footprint analysis: Models can help account for spatial heterogeneity within a a footprint
  * NN could also be trained to calculate/classify footprints!
* u* filtering: Partial derivatives of u* could give thresholds for filtering

# Thank You

Questions?

## Why not use a Random Forest?

RF models are great! ... for classifying discrete objects

* But, it's my view that applying them to continuous data is misguided
* They are [poorly suited](https://github.com/June-Skeeter/NN_Applications) for interpolation and incapable of extrapolation