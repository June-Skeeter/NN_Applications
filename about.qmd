---
title: "A Framework for Applying Neural Networks to Eddy Covariance Data"
# format:
#   html:
jupyter: python3
echo: false
fig-dpi: 300
format:
  revealjs:
    code-fold: true
    controls: true
    navigation-mode: linear
    controls-layout: bottom-right
    controls-tutorial: true
    # margin: 0.05
    # width: 1200
    # css: style.css
    slide-number: true
    show-slide-number: all
    pdfMaxPagesPerSlide: 1
  # pdf:
  #   code-fold: true
  
author:
  - name: Dr. June Skeeter
    # orcid: 0000-0002-7051-343X
    email: june.skeeter@ubc.ca
    url: https://github.com/June-Skeeter
    affiliations:
    - ref: UBC
#   - name: Dr. Sara Knox
#     email: sara.knox@ubc.ca
#     affiliations:
#     - ref: UBC
affiliations:
    - id: UBC
      name: University British Columbia
      department: Department Geography
      address: 1984 West Mall
      city: Vancouver, BC, Canada
      postal-code: V6T 1Z2
keywords: [Eddy Covariance, Micrometeorology, Neural Networks, Modelling]

---

## Eddy Covariance


:::: {.columns}

::: {.column width="50%"}

Semi-continuous, ecosystem-scale measurements of energy, water, and trace gas fluxes.

* Noisy, voluminous data sets
  * Frequent gaps
  * Observational bias
* Well suited for machine learning!

:::

::: {.column width="50%"}

<img src="images/BB1_System.jpg" alt="your-image-description" style="border: 2px solid  black;">

<h3>Burns Bog EC Station<br>Delta, BC</h3>

:::

::::


## Neural Networks

**Universal approximators**: can map any continuous function to an arbitrary degree accuracy.

* Well suited for non-linear, multi-variate response functions
  * Capable **interpolation** and *extrapolation*

* With enough nodes, they will fit **any** pattern in a dataset
  * Often treated as "black boxes"
  * Care must be taken to ensure the patterns are real


## Commonly Cited Limitations


```{python}
#| tbl-colwidths: [40,60]
import pandas as pd

from IPython.display import Markdown
from tabulate import tabulate

df = pd.read_csv('About.csv',sep='|',index_col='Drawback')
Markdown(tabulate(
  df, 
  headers=["Issue", "Solutions"]
))

```

# Objective

Provide a framework for applying NN models to EC data for descriptive analysis and inferential modelling. 
 
* The [github repository](https://github.com/June-Skeeter/NN_Applications) linked to this presentation has functional examples that can be used to apply NN models.
  * Runs in Python and Tensorflow
    * *GPU support not required*

## Example Data

:::: {.columns}

::: {.column width="45%"}

Burns Bog EC station

* Harvested peatland undergoing active restoration
* 8+ years of meteorological & flux (CO<sub>2</sub> and CH<sub>4</sub>) data

:::

::: {.column width="55%"}


<img src="images/BB1.jpg" alt="your-image-description" style="border: 2px solid  black; width: 100%">

:::
::::


## Training Procedures


:::: {.columns}

::: {.column width="70%"}

An iterative process:

* Three way cross-validation
  * Train/validate/test
* Larger ensemble > more robust model
  * N <= 10 suitable for exploration
  * N >=30 will give best performance

:::

::: {.column width="30%"}

<img src="images/NN_Workflows.png" alt="your-image-description">


:::

::::

## Pruning Inputs

Calculate partial first derivative of the output with respect to each input over test data domain.

* **Relative Influence (RI)** of inputs
  * Normalized Sum of squared derivatives (SSD)

* Iteratively remove inputs with RI below a threshold
  * Use a Random scalar input to inform threshold

* Train final model without random scalar

## Before and After Pruning

```{python}
#| label: RI of models
#| layout-ncol: 1
#| warning: False
import pandas as pd
import matplotlib.pyplot as plt
from Scripts import MiscFuncs,PlotHelpers

fig,ax=plt.subplots(1,2,sharey=True,sharex=True)

Base = 'Test'
Name = 'Full_Model'
RI = pd.read_csv(f'Models/{Base}/{Name}/model_RI.csv',index_col=[0])
RI = RI.sort_values(by=f'RI_bar',ascending=True)
PlotHelpers.makeRI_plot(ax[0],RI,Title='Over-Parametrized Model for FCO$_2$')

Name = 'Final_Model'
RI = pd.read_csv(f'Models/{Base}/{Name}/model_RI.csv',index_col=[0])
RI = RI.sort_values(by=f'RI_bar',ascending=True)
PlotHelpers.makeRI_plot(ax[1],RI,Title='Pruned Model for FCO$_2$')
ax[1].set_ylabel('')
plt.tight_layout()

```

## Model Inspection


:::: {.columns}

::: {.column width="50%"}

Plot the model outputs and validation metrics calculated with the test data.

::: {style="font-size: 80%;"}

```{python}
#| label: Validation
#| layout-ncol: 1
#| warning: False
#| tbl-colwidths: [25,75]

import numpy as np
from sklearn import metrics

df = pd.read_csv(f'Models/{Base}/{Name}/model_output.csv',index_col=[0])

unit = '$\mu mol$ $m^{-2}s^{-1}$'

x,y='target','y_bar'
r2 = str(np.round(metrics.r2_score(df[x],df[y]),2))
RMSE = str(np.round(metrics.mean_squared_error(df[x],df[y])**.5,2))+f' {unit}'

m = pd.DataFrame(index=['RMSE','r<sup>2</sup>'],data={'Metrics':[RMSE,r2]})
Markdown(tabulate(
  m, 
  headers=["Metric", "Score"]
))


```

:::

:::

::: {.column width="50%"}


```{python}
#| label: Performance of final model
#| layout-ncol: 1
#| warning: False

df = pd.read_csv(f'Models/{Base}/{Name}/model_output.csv',index_col=[0])

unit = '$\mu mol m^{-2} s^{-1}$'

fig,ax=plt.subplots(figsize=(5,5))
ax = PlotHelpers.make1_1_Plot(ax,df,'target','y_bar',unit=unit)
ax.set_ylabel('NN Estimate')
ax.set_xlabel('EC Observation')
ax.set_title('')
radom_code_to_mask_text=0

```


:::

::::

## Plotting Derivatives
    
Helps ensure model responses are physically plausible

* An **essential step** and **key advantage** of NN models
* Raw derivatives show true feature responses
* Normalized derivatives scaled by input variance
  * Relative effects on common scale
  * What the model "sees"
* 95% confidence intervals around derivatives indicate modeled confidence in relationships


## Partial Derivatives of FCO<sub>2</sub>
 
```{python}
#| label: Derivatives of final model
#| layout-ncol: 1
#| warning: False

Max=4
Top = RI.sort_values(by=f'RI_bar',ascending=False).index[:Max]

cols = 2
npi=len(RI.index)
rows = int(np.ceil(len(Top)/2))

fig,axes=plt.subplots(rows,cols,sharey=True)

axes = axes.flatten()

mod = ''

for i,xi in enumerate(Top):
    df_int = MiscFuncs.byInterval(df,f'{xi}',[f'dy_d{xi}{mod}'],bins=50)
    PlotHelpers.CI_Plot(axes[i],df_int,f'dy_d{xi}{mod}')
    axes[i].set_title(xi)

plt.tight_layout()

axes[i].get_ylabel()

for ax in axes:
    l = ax.get_ylabel()
    ax.set_ylabel(l.split('_norm')[0].replace('_',' / ').replace('y','FCO2'))


radom_code_to_mask_text=0

```

## Normalized Partial Derivatives
 
```{python}
#| label: Normalized Derivatives of final model
#| layout-ncol: 1
#| warning: False

import numpy as np

Max=4
Top = RI.sort_values(by=f'RI_bar',ascending=False).index[:Max]

cols = 2
npi=len(RI.index)
rows = int(np.ceil(len(Top)/2))

fig,axes=plt.subplots(rows,cols,sharey=True,sharex=True)

axes = axes.flatten()

mod = '_norm'

for i,xi in enumerate(Top):
    axes[i].axvspan(0,0,edgecolor='k',linewidth=1.5)
    df_int = MiscFuncs.byInterval(df,f'{xi}{mod}',[f'dy_d{xi}{mod}'],bins=50)
    PlotHelpers.CI_Plot(axes[i],df_int,f'dy_d{xi}{mod}')
    axes[i].set_title(xi)
plt.tight_layout()
axes[i].get_ylabel()
for ax in axes:
    l = ax.get_ylabel()
    ax.set_ylabel(l.split('_')[0]+'/dx')

```


## Next Steps

* Custom NN architecture: Separating input layers may allow us partition fluxes.
  * FCO<sub>2</sub> into GPP and ER
  * FCH<sub>4</sub> into methanogenesis, methanotrophy, transport
* Flux footprints: map response to spatial heterogenity
* Upscaling: in space and time
* u* filtering: partial derivatives could identify u* thresholds for filtering

# Thank You

Questions?

## Why not use a Random Forest?

RF models are great! ... for classifying discrete objects

* But, it's my view that applying them to continuous data is misguided
* They are [poorly suited](https://june-skeeter.github.io/NN_Applications/NN_for_EC.html#evaluate-and-prune-the-model) for interpolation and incapable of extrapolation